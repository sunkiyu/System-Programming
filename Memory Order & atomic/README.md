# Memory Order & atomic

## Memory는 느리다.
* CPU와 RAM은 물리적으로 떨어짐.   
* CPU가 RAM에서 데이터를 읽어오기 위해서는 시간이 오래걸림.   

## 캐시
* RAM이 느리기 떄문에 Cache를 도입   
* 캐시란 CPU 칩 안의 조그마한 메모리   
* 캐시는 CPU에서 연산 수행하는 부분과 거의 붙어있다 싶이해서 읽기/쓰기 속도 매우 빠름.   
* CPU에서 가장 많이 접근하는 메모리는 L1 캐시, 그 다음 자주 접근 L2, L3 캐시 순으로 놓임   
* CPU가 특정 수소 데이터에 접근한다면 일단 캐시에 있는지 확인 후 있으면 읽고 없으면 멤모리까지 갔다온다.   
* 캐시에 있는 데이터를 다시 요청해서 시간 절약하는 것을 Cache Hit라 한다.    
* 반대로 캐시에 요청한 데이터가 없어 메모리까지 갔다오는 것을 Cache miss라 한다   
* 메모리를 읽으면 일단 캐시에 저장 -> 캐시가 다 찼다면 "보통" LRU - Least Recently Used 가장 오랫동안 참조되지 않은 캐시를 날리고 새롭게 기록한다.   
* 따라서 LRU - Least Recently Used에서는 최근데 접근한 데이터를 자주 반복 접근헀다면 유리   


## 코드 재배치 
* 현대의 CPU 한 번에 한 명령어씩 실행하는 것이 아니다.   

### CPU 파이프라이닝 (pipelining)
* 여러 개의 작업을 동시에 실행하는 것을 파이프라이닝(pipelining) 이라고 함.   
* CPU도 마찬가지로 동작, fetch->decode->excecute->write를 동시에 수행.   
* but, 각 명령어 마다 실행 속도는 천차만별   
* 따라서 오랜 시간이 걸리는 명령어가 있으면 다른 명령어들이 밀림 -> 따라서 컴파일러가 CPU 파이프라인을 효율적으로 활용하도록 명령어 재배치함.   
* 물론 전제 조건은 명령어를 재배치해도 최종 결과물은 같아야한다. ( 싱글 스레드일 경우만 보장됨)   
* 따라서 멀티스레드 환경에서는 예상치 못한 결과가 나올 수 있다.   

### 컴파일러뿐만 아니라 CPU도 명령어를 재배치한다.

```cpp
int a =0,b=0;
a=1;//캐시에 없음
b=1;//캐시에 있음 
```
a = 1 의 경우 현재 a가 캐시에 없으므로 매우 오래 걸림.   
b = 1 의 경우 현재 b가 캐시에 있으므로 빠르게 처리 가능.   
따라서, CPU에서 b = 1; 이 a = 1; 보다 먼저 처리될 수 있다.   
따라서, b == 1인데 a == 0 인 순간을 관찰할 수 있다.


